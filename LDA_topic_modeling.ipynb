{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jonathan Keller from https://towardsdatascience.com/building-a-topic-modeling-pipeline-with-spacy-and-gensim-c5dc03ffc619\n",
    "def remove_stopwords(doc):\n",
    "    # Use token.text to return strings, which we'll need for Gensim.\n",
    "    doc = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "    return doc\n",
    "\n",
    "nlp.add_pipe(remove_stopwords, name=\"stopwords\", last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Iowa averaged a pandemic high new Covid infect...\n",
       "1          TheAngel_Plays SweetPxtatoe kiararoyale Megan...\n",
       "2          baxterberrie 4x this many americans died from...\n",
       "3          tonim57601 MarlenaStell Covid-19 Id hate to t...\n",
       "4         Is it covid effect Or something else is going on \n",
       "                                ...                        \n",
       "106674     sibyllete Lol You can have a ballot mailed to...\n",
       "106675     idea of a nonprofit taking a lead role in set...\n",
       "106676     realDonaldTrump Regardless of the outcome of ...\n",
       "106677     Walmart I love the way you have face covering...\n",
       "106678     ARTUZ_teachers RMajongwe ProgressiveOf Nyombw...\n",
       "Name: tweet, Length: 106679, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweets = pd.read_csv('cleaned_covid_tweets.csv')\n",
    "clean_tweets.columns = ['index', 'tweet', 'user', 'id', 'symbols', 'url', 'date']\n",
    "clean_tweets['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = []\n",
    "# Iterates through each article in the corpus.\n",
    "for i in range(len(clean_tweets)):\n",
    "    # Passes that article through the pipeline and adds to a new list.\n",
    "    pr = nlp(clean_tweets.loc[i, 'tweet'])\n",
    "    doc_list.append(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Iowa',\n",
       "  'averaged',\n",
       "  'pandemic',\n",
       "  'high',\n",
       "  'new',\n",
       "  'Covid',\n",
       "  'infections',\n",
       "  'day',\n",
       "  'past',\n",
       "  'week',\n",
       "  'Iowans',\n",
       "  'hospitalized',\n",
       "  'Covid',\n",
       "  'tonight',\n",
       "  'ICU'],\n",
       " [' ',\n",
       "  'TheAngel_Plays',\n",
       "  'SweetPxtatoe',\n",
       "  'kiararoyale',\n",
       "  'MeganPlays',\n",
       "  'Change',\n",
       "  'change',\n",
       "  'going',\n",
       "  's',\n",
       "  'reelected',\n",
       "  'Covid-19',\n",
       "  'cases',\n",
       "  'deaths'],\n",
       " [' ',\n",
       "  'baxterberrie',\n",
       "  '4x',\n",
       "  'americans',\n",
       "  'died',\n",
       "  'covid',\n",
       "  'people',\n",
       "  'took',\n",
       "  'time',\n",
       "  'effort',\n",
       "  'vote',\n",
       "  'waste',\n",
       "  'bastards'],\n",
       " [' ',\n",
       "  'tonim57601',\n",
       "  'MarlenaStell',\n",
       "  'Covid-19',\n",
       "  'd',\n",
       "  'hate',\n",
       "  'think',\n",
       "  'outcome',\n",
       "  'twins',\n",
       "  'monitored',\n",
       "  'closely',\n",
       "  'easy',\n",
       "  'Biden',\n",
       "  've',\n",
       "  'better',\n",
       "  'expected',\n",
       "  'wildest',\n",
       "  'dreams'],\n",
       " ['covid', 'effect', 'going']]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Creates, which is a mapping of word IDs to words.\n",
    "words = corpora.Dictionary(doc_list)\n",
    "# Turns each document into a bag of words.\n",
    "corpus = [words.doc2bow(doc) for doc in doc_list]\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=words,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=2,\n",
    "                                           update_every=1,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.039*\"mask\" + 0.018*\"wear\" + 0.014*\"masks\" + 0.012*\"wearing\" + 0.011*\"face\" + 0.008*\"üôè\" + 0.008*\"Dalton\" + 0.007*\"active\" + 0.007*\"mental\" + 0.007*\"Andy\" + 0.006*\"article\" + 0.006*\"miss\" + 0.006*\"ill\" + 0.006*\"cold\" + 0.006*\"beds\" + 0.005*\"common\" + 0.005*\"spike\" + 0.005*\"Cowboys\" + 0.005*\"amid\" + 0.005*\"experience\" + 0.005*\"distance\" + 0.005*\"teams\" + 0.004*\"child\" + 0.004*\"worked\" + 0.004*\"services\" + 0.004*\"5\" + 0.004*\"virtual\" + 0.004*\"updates\" + 0.004*\"Find\" + 0.004*\"PPE\"'),\n",
       " (1,\n",
       "  '0.017*\"üòÇ\" + 0.014*\"CDC\" + 0.011*\"ü§£\" + 0.009*\"quarantine\" + 0.009*\"important\" + 0.009*\"piersmorgan\" + 0.008*\"highest\" + 0.008*\"low\" + 0.007*\"üá∫\" + 0.006*\"üá∏\" + 0.006*\"Canada\" + 0.006*\"Thank\" + 0.006*\"worst\" + 0.006*\"sense\" + 0.006*\"contact\" + 0.005*\"communities\" + 0.005*\"higher\" + 0.005*\"allowed\" + 0.005*\"England\" + 0.005*\"üò∑\" + 0.004*\"rates\" + 0.004*\"üèª\" + 0.004*\"vs\" + 0.004*\"lack\" + 0.004*\"poor\" + 0.004*\"guy\" + 0.004*\"experts\" + 0.004*\"Learn\" + 0.004*\"info\" + 0.004*\"poll\"'),\n",
       " (2,\n",
       "  '0.022*\"U\" + 0.019*\"Tuesday\" + 0.018*\"symptoms\" + 0.018*\"County\" + 0.017*\"News\" + 0.016*\"Deaths\" + 0.016*\"Vote\" + 0.015*\"Cases\" + 0.014*\"S\" + 0.009*\"Total\" + 0.008*\"‚ù§\" + 0.008*\"ballot\" + 0.008*\"Public\" + 0.007*\"PM\" + 0.007*\"morning\" + 0.006*\"industry\" + 0.006*\"York\" + 0.006*\"Friday\" + 0.006*\"additional\" + 0.006*\"Department\" + 0.005*\"cast\" + 0.005*\"scientists\" + 0.005*\"bringing\" + 0.005*\"ü•∫\" + 0.005*\"beginning\" + 0.004*\"diagnosed\" + 0.004*\"ward\" + 0.004*\"multiple\" + 0.004*\"rally\" + 0.004*\"New\"'),\n",
       " (3,\n",
       "  '0.061*\"positive\" + 0.055*\"test\" + 0.033*\"tested\" + 0.029*\"testing\" + 0.018*\"tests\" + 0.014*\"tomorrow\" + 0.013*\"negative\" + 0.012*\"na\" + 0.012*\"gon\" + 0.011*\"staff\" + 0.010*\"players\" + 0.009*\"local\" + 0.008*\"available\" + 0.008*\"false\" + 0.006*\"research\" + 0.006*\"BorisJohnson\" + 0.006*\"elderly\" + 0.005*\"officials\" + 0.005*\"amazing\" + 0.005*\"üò±\" + 0.005*\"released\" + 0.005*\"service\" + 0.005*\"doctor\" + 0.005*\"Johnson\" + 0.005*\"Time\" + 0.004*\"companies\" + 0.004*\"send\" + 0.004*\"Broncos\" + 0.004*\"wanna\" + 0.004*\"Elway\"'),\n",
       " (4,\n",
       "  '0.019*\"hospitals\" + 0.015*\"rules\" + 0.013*\"gt\" + 0.012*\"confirmed\" + 0.012*\"infection\" + 0.011*\"NHS\" + 0.011*\"nursing\" + 0.008*\"covid19\" + 0.008*\"Monday\" + 0.008*\"toll\" + 0.007*\"Watch\" + 0.006*\"Europe\" + 0.006*\"near\" + 0.006*\"restaurants\" + 0.006*\"guidance\" + 0.006*\"T\" + 0.006*\"ü§¨\" + 0.005*\"wife\" + 0.005*\"transmission\" + 0.005*\"Twitter\" + 0.005*\"SARS\" + 0.005*\"üòî\" + 0.005*\"breaking\" + 0.005*\"records\" + 0.005*\"severe\" + 0.004*\"gives\" + 0.004*\"causes\" + 0.004*\"turned\" + 0.004*\"era\" + 0.004*\"round\"'),\n",
       " (5,\n",
       "  '0.075*\" \" + 0.063*\"Covid\" + 0.059*\"COVID\" + 0.027*\"people\" + 0.022*\"amp\" + 0.020*\"Trump\" + 0.013*\"Covid-19\" + 0.009*\"realDonaldTrump\" + 0.008*\"vote\" + 0.007*\"virus\" + 0.007*\"died\" + 0.007*\"Biden\" + 0.006*\"death\" + 0.006*\"care\" + 0.006*\"lockdown\" + 0.005*\"Ô∏è\" + 0.005*\"says\" + 0.005*\"country\" + 0.005*\"world\" + 0.004*\"years\" + 0.004*\"Americans\" + 0.004*\"spread\" + 0.004*\"deaths\" + 0.004*\"response\" + 0.004*\"economy\" + 0.004*\"die\" + 0.004*\"vaccine\" + 0.004*\"lost\" + 0.004*\"health\" + 0.004*\"plan\"'),\n",
       " (6,\n",
       "  '0.021*\"UK\" + 0.016*\"November\" + 0.011*\"seen\" + 0.009*\"measures\" + 0.008*\"wave\" + 0.008*\"rise\" + 0.008*\"forward\" + 0.008*\"Heres\" + 0.008*\"watch\" + 0.007*\"Great\" + 0.006*\"Boris\" + 0.006*\"ElectionDay\" + 0.006*\"share\" + 0.006*\"John\" + 0.005*\"allow\" + 0.005*\"Brexit\" + 0.005*\"learning\" + 0.005*\"group\" + 0.005*\"weekend\" + 0.004*\"winter\" + 0.004*\"ways\" + 0.004*\"City\" + 0.004*\"closing\" + 0.004*\"football\" + 0.004*\"üëá\" + 0.004*\"minutes\" + 0.004*\"nation\" + 0.004*\"access\" + 0.004*\"October\" + 0.004*\"nice\"'),\n",
       " (7,\n",
       "  '0.067*\"covid\" + 0.054*\"nt\" + 0.052*\" \" + 0.016*\"s\" + 0.016*\"m\" + 0.015*\"like\" + 0.010*\"know\" + 0.010*\"time\" + 0.010*\"today\" + 0.009*\"going\" + 0.009*\"think\" + 0.009*\"got\" + 0.009*\"day\" + 0.008*\"year\" + 0.008*\"ve\" + 0.008*\"election\" + 0.007*\"need\" + 0.007*\"work\" + 0.006*\"good\" + 0.006*\"numbers\" + 0.006*\"said\" + 0.006*\"way\" + 0.006*\"want\" + 0.006*\"right\" + 0.005*\"help\" + 0.005*\"getting\" + 0.005*\"home\" + 0.005*\"week\" + 0.005*\"days\" + 0.005*\"flu\"'),\n",
       " (8,\n",
       "  '0.017*\"latest\" + 0.016*\"season\" + 0.010*\"single\" + 0.010*\"waiting\" + 0.010*\"distancing\" + 0.009*\"recently\" + 0.008*\"Government\" + 0.008*\"YouTube\" + 0.007*\"terrible\" + 0.007*\"car\" + 0.007*\"School\" + 0.007*\"patient\" + 0.007*\"cancelled\" + 0.007*\"games\" + 0.007*\"changed\" + 0.006*\"Halloween\" + 0.006*\"nypost\" + 0.006*\"couple\" + 0.006*\"safety\" + 0.006*\"fighting\" + 0.005*\"straight\" + 0.005*\"warns\" + 0.005*\"scandal\" + 0.005*\"Pence\" + 0.005*\"despair\" + 0.005*\"holiday\" + 0.005*\"mandatory\" + 0.005*\"spent\" + 0.004*\"December\" + 0.004*\"corner\"'),\n",
       " (9,\n",
       "  '0.091*\"COVID-19\" + 0.030*\"cases\" + 0.023*\"new\" + 0.021*\"deaths\" + 0.017*\"pandemic\" + 0.011*\"COVID19\" + 0.009*\"patients\" + 0.008*\"coronavirus\" + 0.007*\"number\" + 0.007*\"weeks\" + 0.007*\"rate\" + 0.007*\"public\" + 0.006*\"New\" + 0.006*\"health\" + 0.006*\"case\" + 0.006*\"reported\" + 0.006*\"Health\" + 0.005*\"Election\" + 0.005*\"restrictions\" + 0.005*\"PENN\" + 0.005*\"record\" + 0.005*\"Day\" + 0.005*\"data\" + 0.005*\"NFL\" + 0.005*\"Covid_19\" + 0.005*\"daily\" + 0.004*\"Dr\" + 0.004*\"report\" + 0.004*\"schools\" + 0.004*\"total\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics(num_words = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It feels like there are a few topics in here that make sense, but I'm not sure I \"see\" what a lot of it is. This does very much feel like it's essentially a snapshot of the specific day when I pulled the tweets.\n",
    "\n",
    "Next steps:\n",
    "1. Grab another chunk of tweets using Tweepy; possibly grab a third chunk from another point or three in time.\n",
    "2. Figure out model perplexity/coherence so I know when I'm improving things; implement pyLDAvis so I can see how things are looking.\n",
    "3. Replace the letter icons used for flag emojis (üá∫ and üá∏ with us_flag, since that's what they represent)\n",
    "4. Lowercase words - seems unlikely that capitalization is going to make a difference\n",
    "5. Leave ' @\\w' in place; delete ' @ ' ( == ' at ' == stop word)? Seems possible that some highly relevant Twitter handles may mean different things as hastag/plaintext vs handle.\n",
    "6. How does LDA train with this cleaning scheme perform pre-removal of stop words?\n",
    "7. Pull out stop words, including days of the week and day referents (today, tomorrow, yesterday)\n",
    "8. How does LDA perform after stop words are removed?\n",
    "9. Compare lemmatized with non-lemmatized results - is avoiding lemmatization actually making any difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
